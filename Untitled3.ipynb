{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985954c9-cf3f-41b7-a72a-db4ba28e2668",
   "metadata": {
    "editable": false,
    "tags": [
     "o9_ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is NOT editable. Overwrite variables on your own discretion.\n",
    "# Any changes other than the script code will NOT BE SAVED!\n",
    "# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b63a3da-7490-4060-ac6b-283a5d46c336",
   "metadata": {
    "tags": [
     "o9_ignore"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Warning: Master yarn-cluster is deprecated since 2.0. Please use master \"yarn\" with specified deploy mode instead.\n",
      "21/11/26 10:08:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/11/26 10:08:14 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "21/11/26 10:08:14 INFO RMProxy: Connecting to ResourceManager at hadoop-qa-o9-scus-master01/10.190.3.30:8050\n",
      "21/11/26 10:08:14 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "21/11/26 10:08:14 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.4.0-315/0/resource-types.xml\n",
      "21/11/26 10:08:14 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (45056 MB per container)\n",
      "21/11/26 10:08:14 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "21/11/26 10:08:14 INFO Client: Setting up container launch context for our AM\n",
      "21/11/26 10:08:14 INFO Client: Setting up the launch environment for our AM container\n",
      "21/11/26 10:08:14 INFO Client: Preparing resources for our AM container\n",
      "21/11/26 10:08:15 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://o9azqa/hdp/apps/3.1.4.0-315/spark2/spark2-hdp-yarn-archive.tar.gz\n",
      "21/11/26 10:08:15 INFO Client: Source and destination file systems are the same. Not copying hdfs://o9azqa/hdp/apps/3.1.4.0-315/spark2/spark2-hdp-yarn-archive.tar.gz\n",
      "21/11/26 10:08:15 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://o9azqa/hdp/apps/3.1.4.0-315/spark2/spark2-hdp-hive-archive.tar.gz\n",
      "21/11/26 10:08:15 INFO Client: Source and destination file systems are the same. Not copying hdfs://o9azqa/hdp/apps/3.1.4.0-315/spark2/spark2-hdp-hive-archive.tar.gz\n",
      "21/11/26 10:08:15 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-api-0.5.0.3.1.4.0-315.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/livy-api-0.5.0.3.1.4.0-315.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-rsc-0.5.0.3.1.4.0-315.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/livy-rsc-0.5.0.3.1.4.0-315.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/netty-all-4.1.17.Final.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/commons-codec-1.9.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-core_2.11-0.5.0.3.1.4.0-315.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/livy-core_2.11-0.5.0.3.1.4.0-315.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-repl_2.11-0.5.0.3.1.4.0-315.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/livy-repl_2.11-0.5.0.3.1.4.0-315.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-4.2.1.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/datanucleus-api-jdo-4.2.1.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-core-4.1.6.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/datanucleus-core-4.1.6.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-4.1.7.jar -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/datanucleus-rdbms-4.1.7.jar\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/etc/spark2/3.1.4.0-315/0/hive-site.xml -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/hive-site.xml\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/R/lib/sparkr.zip#sparkr -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/sparkr.zip\n",
      "21/11/26 10:08:16 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/pyspark.zip\n",
      "21/11/26 10:08:17 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/py4j-0.10.7-src.zip\n",
      "21/11/26 10:08:17 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/pyspark.zip added multiple times to distributed cache\n",
      "21/11/26 10:08:17 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n",
      "21/11/26 10:08:17 INFO Client: Uploading resource file:/tmp/spark-7c9cedaa-625f-4b73-a0bc-67f07f1b2f71/__spark_conf__6411018847307972570.zip -> hdfs://o9azqa/user/livy/.sparkStaging/application_1636607885619_0060/__spark_conf__.zip\n",
      "21/11/26 10:08:17 INFO SecurityManager: Changing view acls to: livy\n",
      "21/11/26 10:08:17 INFO SecurityManager: Changing modify acls to: livy\n",
      "21/11/26 10:08:17 INFO SecurityManager: Changing view acls groups to: \n",
      "21/11/26 10:08:17 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/11/26 10:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "21/11/26 10:08:17 INFO Client: Submitting application application_1636607885619_0060 to ResourceManager\n",
      "21/11/26 10:08:17 INFO YarnClientImpl: Submitted application application_1636607885619_0060\n",
      "21/11/26 10:08:17 INFO Client: Application report for application_1636607885619_0060 (state: ACCEPTED)\n",
      "21/11/26 10:08:17 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1637921297402\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop-qa-o9-scus-master01:8088/proxy/application_1636607885619_0060/\n",
      "\t user: livy\n",
      "21/11/26 10:08:17 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/11/26 10:08:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c9cedaa-625f-4b73-a0bc-67f07f1b2f71\n",
      "21/11/26 10:08:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-4e9f8b4a-bbf1-49d0-a30d-d111024ff38c\n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1636607885619_0060 failed 1 times (global limit =2; local limit is =1) due to AM Container for appattempt_1636607885619_0060_000001 exited with  exitCode: -104\n",
      "Failing this attempt.Diagnostics: [2021-11-26 10:11:20.667]Container [pid=5459,containerID=container_e16_1636607885619_0060_01_000001] is running 10272768B beyond the 'PHYSICAL' memory limit. Current usage: 2.0 GB of 2 GB physical memory used; 5.3 GB of 4.2 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_e16_1636607885619_0060_01_000001 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 6131 5482 5459 5459 (python) 303 293 2151354368 283221 /opt/conda/envs/genieaz_pyplatformhivetest/bin/python /hadoop/hdfs/data/hadoop/yarn/local/usercache/livy/appcache/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/tmp/6937708951975723561 \n",
      "\t|- 5482 5459 5459 5459 (java) 4393 256 3378180096 243204 /usr/jdk64/jdk1.8.0_112/bin/java -server -Xmx1024m -Djava.io.tmpdir=/hadoop/hdfs/data/hadoop/yarn/local/usercache/livy/appcache/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/tmp -Dhdp.version=3.1.4.0-315 -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class org.apache.livy.rsc.driver.RSCDriverBootstrapper --properties-file /hadoop/hdfs/data/hadoop/yarn/local/usercache/livy/appcache/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/__spark_conf__/__spark_conf__.properties \n",
      "\t|- 5459 5457 5459 5459 (bash) 0 1 118181888 371 /bin/bash -c LD_LIBRARY_PATH=/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64: /usr/jdk64/jdk1.8.0_112/bin/java -server -Xmx1024m -Djava.io.tmpdir=/hadoop/hdfs/data/hadoop/yarn/local/usercache/livy/appcache/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/tmp -Dhdp.version=3.1.4.0-315 -Dspark.yarn.app.container.log.dir=/mnt/resource/hadoop/yarn/log/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class 'org.apache.livy.rsc.driver.RSCDriverBootstrapper' --properties-file /hadoop/hdfs/data/hadoop/yarn/local/usercache/livy/appcache/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/__spark_conf__/__spark_conf__.properties 1> /mnt/resource/hadoop/yarn/log/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/stdout 2> /mnt/resource/hadoop/yarn/log/application_1636607885619_0060/container_e16_1636607885619_0060_01_000001/stderr \n",
      "\n",
      "[2021-11-26 10:11:20.684]Container killed on request. Exit code is 143\n",
      "[2021-11-26 10:11:20.694]Container exited with a non-zero exit code 143. \n",
      "For more detailed output, check the application tracking page: http://hadoop-qa-o9-scus-master01:8088/cluster/app/application_1636607885619_0060 Then click on links to logs of each attempt.\n",
      ". Failing the application.\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "from o9_ibpl_magics import spark_ibpl\n",
    "input_df = spark_ibpl('select (Version.[Version Name]*Product.[Product].[196426]*Time.FiscalWeek*SalesAccount.[Account]*Location.[Location]*{Measure.[DPSellOutUnitsActuals],Measure.[Mean Pricing Save PCT],Measure.[Placement Count],Measure.[Promotion Count],Measure.[DPSellOutPrice]});',spark)\n",
    "predict_df = spark_ibpl('select (Version.[Version Name]*Product.[Product].[196426]*Time.FiscalWeek*SalesAccount.[Account]*Location.[Location]*{Measure.[DPSellOutUnitsActuals],Measure.[Mean Pricing Save PCT],Measure.[Placement Count],Measure.[Promotion Count],Measure.[DPSellOutPrice]});',spark)\n",
    "\n",
    "\n",
    "# Initialize the O9DataLake with the input parameters and dataframes\n",
    "# Data can be accessed with O9DataLake.get(<Input Name>)\n",
    "# Overwritten values will not be reflected in the O9DataLake after initialization\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake\n",
    "O9DataLake.init_data_lake({'input_df': input_df, 'predict_df': predict_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ce7eb-c8e4-4916-b508-78fd19cb8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "\"\"\"\n",
    "exec plugin instance [PySpartPlugin] for measures {[DPSellOutUnitsActuals]} using scope ([Product].[Product].Filter(#.Name in {[196426],[208821]}) * [Time].[Day] * [Version].[Version Name].[CurrentWorkingView]) using arguments {(NumExecutors, 1), (ExecutorMemory, \"1G\"), (DriverMemory, \"1G\") ,(PythonPath, \"/opt/conda/envs/sandbox_pyplatformtest/bin/python\")};\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "print('>>>>>>input_df', input_df.head(2))\n",
    "\n",
    "out_df = None\n",
    "\n",
    "if predict_df is not None and len(predict_df.index) > 0:\n",
    "    x = input_df[['Mean Pricing Save PCT', 'Placement Count', 'Promotion Count', 'DPSellOutPrice']]\n",
    "    y = input_df[['DPSellOutUnitsActuals']]\n",
    "    x = x.fillna(0).values\n",
    "    y = y.fillna(0).values\n",
    "\n",
    "    clf = tree.DecisionTreeRegressor()\n",
    "    clf = clf.fit(x, y)\n",
    "\n",
    "    predict = predict_df[['Mean Pricing Save PCT', 'Placement Count', 'Promotion Count', 'DPSellOutPrice']].fillna(0).values\n",
    "    predict_values = clf.predict(predict)\n",
    "    predict_df[\"DPSellOutUnitsFcst\"] = predict_values\n",
    "    #out_df = pd.concat([predict_keys, out_df_part], axis=1)\n",
    "    out_df=predict_df[['Time.[FiscalWeek]', 'Product.[Product]', 'Version.[Version Name]', 'SalesAccount.[Account]','Location.[Location]','DPSellOutUnitsFcst']]\n",
    "\n",
    "else:\n",
    "    out_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4ae46-e373-48f8-aac8-e4b5ab02993a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[pyplatformhivetest] Tenant Conda Environment",
   "language": "python",
   "name": "genieaz_pyplatformhivetest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
